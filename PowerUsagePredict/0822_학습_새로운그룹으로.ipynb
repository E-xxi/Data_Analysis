{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f71d8fd6-42ad-43d8-8289-9c1af7323e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import koreanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2e12f-0f23-4c0e-ab20-60f2c254116f",
   "metadata": {},
   "source": [
    "## 앙상블 단계별 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "698ead5c-78b9-44a7-a782-9d94ea40b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import xgboost as xgb\n",
    "\n",
    "# ======================\n",
    "# 0) 경로 & 상수\n",
    "# ======================\n",
    "TRAIN_PATH = \"Data/train_0822_changed.csv\"\n",
    "TEST_PATH  = \"Data/test_0822_changed.csv\"\n",
    "SUB_PATH   = \"Data/sample_submission.csv\"\n",
    "\n",
    "KEYS   = [\"num_date_time\", \"건물번호\"]\n",
    "TARGET = \"전력소비량(kWh)\"\n",
    "BASE_DROP_COMMON = [\"num_date_time\", \"일시\"]  # 공통 시간 문자열\n",
    "BASE_DROP_TRAIN  = [\"건물유형\", TARGET]       # 모델1·3 (건물유형 split)에서 drop\n",
    "BASE_DROP_TEST   = [\"건물유형\"]\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test  = pd.read_csv(TEST_PATH)\n",
    "sub   = pd.read_csv(SUB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "148da813-50dd-4960-81c0-b28116b75cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    denom = np.where(denom == 0, 1, denom)\n",
    "    return np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100\n",
    "\n",
    "def coerce_numeric_objects(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"object/category 컬럼을 안전하게 숫자로 변환 (숫자/기호 혼재 대응)\"\"\"\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if str(out[c].dtype) in [\"object\", \"category\"]:\n",
    "            out[c] = pd.to_numeric(\n",
    "                out[c].astype(str).str.replace(r\"[^\\d\\.\\-eE]\", \"\", regex=True),\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "    return out\n",
    "\n",
    "def reorder_like(X_to_fix, X_ref):\n",
    "    \"\"\"X_to_fix를 X_ref의 컬럼 순서/구성에 맞춘다. 누락은 NaN으로 채움.\"\"\"\n",
    "    return X_to_fix.reindex(columns=X_ref.columns, fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "185d692c-1cb9-4fb2-8304-88eb2e26264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 수집 \n",
    "# ======================\n",
    "# 모델 1: CatBoost (건물유형별)\n",
    "val_oofs_1, test_preds_1 = [], []\n",
    "# 모델 2: XGBoost (cluster_id별)\n",
    "val_oofs_2, test_preds_2 = [], []\n",
    "# 모델 3: XGBoost (건물유형별)\n",
    "val_oofs_3, test_preds_3 = [], []\n",
    "\n",
    "val_scores_1 = {}\n",
    "val_scores_2 = {}\n",
    "val_scores_3 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33dd9ca9-4e08-463a-81cc-4beb99c90707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[호텔] Base=5.8292, Pruned=5.8522 -> Use Base\n",
      "[상용] Base=1.6944, Pruned=1.6775 -> Use Pruned\n",
      "[병원] Base=2.6731, Pruned=2.6230 -> Use Pruned\n",
      "[CLUSTER10] Base=7.6803, Pruned=7.7955 -> Use Base\n",
      "[학교] Base=2.5522, Pruned=2.5531 -> Use Base\n",
      "[CLUSTER11] Base=5.5509, Pruned=5.5123 -> Use Pruned\n",
      "[건물기타] Base=4.2859, Pruned=4.1811 -> Use Pruned\n",
      "[아파트] Base=2.9066, Pruned=2.8773 -> Use Pruned\n",
      "[연구소] Base=2.9307, Pruned=2.8718 -> Use Pruned\n",
      "[백화점] Base=5.2557, Pruned=5.2003 -> Use Pruned\n",
      "[IDC(전화국)] Base=1.2351, Pruned=1.1797 -> Use Pruned\n",
      "[공공] Base=5.2720, Pruned=5.1687 -> Use Pruned\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 설정 (그대로 사용) ---\n",
    "params_cb = dict(\n",
    "    loss_function=\"RMSE\", \n",
    "    eval_metric=\"SMAPE\",\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=3.0,\n",
    "    random_seed=42,\n",
    "    iterations=2500,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=200,\n",
    "    verbose=False,\n",
    "    allow_writing_files=False\n",
    ")\n",
    "\n",
    "def cat_features_of(X: pd.DataFrame):\n",
    "    return [c for c in X.columns if str(X[c].dtype) in (\"object\", \"category\")]\n",
    "\n",
    "def feature_importance_series(model: CatBoostRegressor, pool: Pool, cols):\n",
    "    imp = model.get_feature_importance(pool, type=\"PredictionValuesChange\")\n",
    "    return pd.Series(imp, index=cols).sort_values(ascending=True)\n",
    "\n",
    "def prune_columns_by_bottom_fraction(cols, fi_series, bottom_frac=0.15):\n",
    "    n = len(cols)\n",
    "    k = max(1, int(np.ceil(n * bottom_frac)))           # 최소 1개는 제거\n",
    "    drop_list = list(fi_series.index[:k])               # 중요도 낮은 순으로 k개\n",
    "    keep_cols = [c for c in cols if c not in drop_list]\n",
    "    # 과도 제거 방지: 최소 특성 개수 보장 (원하면 임계값 조정)\n",
    "    min_keep = max(10, int(np.ceil(n * 0.5)))           # 최소 10개 또는 절반\n",
    "    if len(keep_cols) < min_keep:                       # 너무 많이 지웠으면 상위만 남김\n",
    "        keep_cols = list(fi_series.index[k:])           # 남은 상위 중요도\n",
    "    return keep_cols, drop_list\n",
    "\n",
    "building_types = train[\"train_group3\"].dropna().unique()\n",
    "\n",
    "for btype in building_types:\n",
    "    tr_sub = train[train[\"train_group3\"] == btype].copy()\n",
    "    te_sub = test [test [\"train_group3\"] == btype].copy()\n",
    "    if len(tr_sub) == 0:\n",
    "        continue\n",
    "\n",
    "    # --- 피처/타깃 분리 ---\n",
    "    drop_cols_tr = BASE_DROP_COMMON + BASE_DROP_TRAIN\n",
    "    X_all = tr_sub.drop(columns=[c for c in drop_cols_tr if c in tr_sub.columns], errors=\"ignore\")\n",
    "    y_all = tr_sub[TARGET].astype(float)\n",
    "\n",
    "    # --- 고정 검증 분할(동일 분할로 공정 비교) ---\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "    cat_cols = cat_features_of(X_tr)\n",
    "\n",
    "    # --- 1) 베이스 학습 ---\n",
    "    pool_tr  = Pool(X_tr, y_tr, cat_features=cat_cols)\n",
    "    pool_val = Pool(X_val, y_val, cat_features=cat_cols)\n",
    "    m_base = CatBoostRegressor(**params_cb)\n",
    "    m_base.fit(pool_tr, eval_set=pool_val, use_best_model=True)\n",
    "    pred_base = m_base.predict(pool_val)\n",
    "    smape_base = smape(y_val.values, pred_base)\n",
    "\n",
    "    # --- 1차 FI 산출 & 하위 15% 제거 ---\n",
    "    fi_series = feature_importance_series(m_base, pool_tr, X_tr.columns)\n",
    "    keep_cols, dropped_cols = prune_columns_by_bottom_fraction(list(X_tr.columns), fi_series, bottom_frac=0.15)\n",
    "\n",
    "    # --- 2) Pruned 학습(동일 분할) ---\n",
    "    X_tr2  = X_tr[keep_cols].copy()\n",
    "    X_val2 = X_val[keep_cols].copy()\n",
    "    cat_cols2 = [c for c in keep_cols if c in cat_cols]\n",
    "\n",
    "    pool_tr2  = Pool(X_tr2, y_tr, cat_features=cat_cols2)\n",
    "    pool_val2 = Pool(X_val2, y_val, cat_features=cat_cols2)\n",
    "\n",
    "    m_pruned = CatBoostRegressor(**params_cb)\n",
    "    m_pruned.fit(pool_tr2, eval_set=pool_val2, use_best_model=True)\n",
    "    pred_pruned = m_pruned.predict(pool_val2)\n",
    "    smape_pruned = smape(y_val.values, pred_pruned)\n",
    "\n",
    "    # --- 더 좋은 쪽 선택 ---\n",
    "    use_pruned = smape_pruned + 1e-12 < smape_base\n",
    "    best_smape = smape_pruned if use_pruned else smape_base\n",
    "    val_scores_1[btype] = best_smape\n",
    "    print(f\"[{btype}] Base={smape_base:.4f}, Pruned={smape_pruned:.4f} -> Use {'Pruned' if use_pruned else 'Base'}\")\n",
    "\n",
    "    # --- OOF 저장(선택된 쪽) ---\n",
    "    val_keys = tr_sub.loc[X_val.index, KEYS].reset_index(drop=True)  # 동일 인덱스 기준\n",
    "    oof_df = val_keys.copy()\n",
    "    oof_df[\"y_true\"] = y_val.reset_index(drop=True).values\n",
    "    oof_df[\"pred_1\"] = (pred_pruned if use_pruned else pred_base)\n",
    "    val_oofs_1.append(oof_df)\n",
    "\n",
    "    # --- 최종 재학습: 선택된 피처로 전체 train 사용 + best_iteration 반영 ---\n",
    "    if use_pruned:\n",
    "        cols_final = keep_cols\n",
    "        best_iter  = m_pruned.get_best_iteration() or params_cb[\"iterations\"]\n",
    "    else:\n",
    "        cols_final = list(X_tr.columns)\n",
    "        best_iter  = m_base.get_best_iteration() or params_cb[\"iterations\"]\n",
    "\n",
    "    # 전체 train에서 같은 컬럼만 사용\n",
    "    X_full = X_all[cols_final].copy()\n",
    "    cat_cols_final = [c for c in cols_final if c in cat_cols]  # 카테고리 컬럼 교차\n",
    "    pool_full = Pool(X_full, y_all, cat_features=cat_cols_final)\n",
    "\n",
    "    # 동일 파라미터 + best_iter로 재학습\n",
    "    params_final = params_cb.copy()\n",
    "    params_final[\"iterations\"] = int(best_iter)\n",
    "    m_final = CatBoostRegressor(**params_final)\n",
    "    # 참고: 여기서는 eval_set 없이 고정 iteration 훈련 (use_best_model 불필요)\n",
    "    m_final.fit(pool_full, verbose=False)\n",
    "\n",
    "    # --- TEST 예측 ---\n",
    "    X_test = te_sub.drop(columns=[c for c in BASE_DROP_COMMON + BASE_DROP_TEST if c in te_sub.columns], errors=\"ignore\")\n",
    "    # 열 순서 맞추기\n",
    "    X_test = X_test.reindex(columns=cols_final, fill_value=0)\n",
    "    pool_te = Pool(X_test, cat_features=cat_cols_final)\n",
    "    te_pred = m_final.predict(pool_te)\n",
    "\n",
    "    test_preds_1.append(te_sub[KEYS].assign(pred_1=te_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "843d03b8-3fed-4106-b86f-acc0b6f966cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[호텔] Validation SMAPE: 5.5505 | best_iter=None\n",
      "[상용] Validation SMAPE: 1.7613 | best_iter=None\n",
      "[병원] Validation SMAPE: 2.5763 | best_iter=None\n",
      "[CLUSTER10] Validation SMAPE: 5.1974 | best_iter=None\n",
      "[학교] Validation SMAPE: 2.8666 | best_iter=None\n",
      "[CLUSTER11] Validation SMAPE: 5.5608 | best_iter=None\n",
      "[건물기타] Validation SMAPE: 4.1807 | best_iter=None\n",
      "[아파트] Validation SMAPE: 3.6995 | best_iter=None\n",
      "[연구소] Validation SMAPE: 3.1672 | best_iter=None\n",
      "[백화점] Validation SMAPE: 4.2196 | best_iter=None\n",
      "[IDC(전화국)] Validation SMAPE: 0.7764 | best_iter=None\n",
      "[공공] Validation SMAPE: 5.0731 | best_iter=None\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 모델 2 — XGBoost (train_group별)\n",
    "# ======================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 숫자화 유틸\n",
    "def to_numeric_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        if str(out[c].dtype) in [\"object\", \"category\"]:\n",
    "            out[c] = pd.to_numeric(\n",
    "                out[c].astype(str).str.replace(r\"[^\\d\\.\\-eE]\", \"\", regex=True),\n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "    return out\n",
    "\n",
    "params_xgb = dict(\n",
    "    n_estimators=700,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=\"hist\",   # GPU면 \"gpu_hist\"\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"mae\"\n",
    ")\n",
    "\n",
    "# 중요도 기반 제외 컬럼(선택)\n",
    "drop_cols = ['T2', 'T_x_rain', 'RH2', '습도(%)', '풍속(m/s)', 'is_rain', 'wind_level', '강수량(mm)', '기온(°C)', 'ess_hours']\n",
    "\n",
    "GROUP_COL = \"train_group3\"   # 위에서 만든 그룹키(유형|클러스터)\n",
    "groups = train[GROUP_COL].dropna().unique()\n",
    "\n",
    "for gkey in groups:\n",
    "    tr_sub = train[train[GROUP_COL] == gkey].copy()\n",
    "    te_sub = test [test [GROUP_COL] == gkey].copy()\n",
    "    if len(tr_sub) == 0:\n",
    "        continue\n",
    "\n",
    "    # 1) Feature/Target\n",
    "    drop_cols_tr = BASE_DROP_COMMON + [\"cluster_id\", \"건물번호\", TARGET] + [GROUP_COL]\n",
    "    X_all = tr_sub.drop(columns=[c for c in drop_cols_tr if c in tr_sub.columns], errors=\"ignore\")\n",
    "    y_all = tr_sub[TARGET].astype(float)\n",
    "\n",
    "    # 숫자화 + 결측 보정\n",
    "    X_all = to_numeric_df(X_all).fillna(0.0)\n",
    "\n",
    "    # 2) Split (고정 시드로 공정한 비교)\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_all, y_all, test_size=0.2, random_state=38)\n",
    "\n",
    "    # 3) 학습 (조기 종료)\n",
    "    m2 = xgb.XGBRegressor(**params_xgb)\n",
    "    m2.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    best_iter = getattr(m2, \"best_iteration\", None)\n",
    "    # 4) 검증 예측 + 점수\n",
    "    val_pred = m2.predict(X_val, iteration_range=(0, best_iter+1) if best_iter is not None else None)\n",
    "    score = smape(y_val.values, val_pred)\n",
    "\n",
    "    val_scores_2[gkey] = score\n",
    "    print(f\"[{gkey}] Validation SMAPE: {score:.4f} | best_iter={best_iter}\")\n",
    "\n",
    "    # 5) OOF 수집 (키 인덱스 정합성 유지)\n",
    "    val_keys = tr_sub.loc[X_val.index, KEYS].reset_index(drop=True)\n",
    "    oof_df = val_keys.copy()\n",
    "    oof_df[\"y_true\"] = y_val.reset_index(drop=True).values\n",
    "    oof_df[\"pred_2\"] = val_pred\n",
    "    val_oofs_2.append(oof_df)\n",
    "\n",
    "    # 6) TEST 예측\n",
    "    X_test = te_sub.drop(columns=[c for c in BASE_DROP_COMMON + [\"cluster_id\", GROUP_COL] if c in te_sub.columns], errors=\"ignore\")\n",
    "    X_test = to_numeric_df(X_test).fillna(0.0)\n",
    "    # 학습 컬럼 정합성(순서/누락) 맞추기\n",
    "    X_test = X_test.reindex(columns=X_tr.columns, fill_value=0.0)\n",
    "\n",
    "    te_pred = m2.predict(X_test, iteration_range=(0, best_iter+1) if best_iter is not None else None)\n",
    "    test_preds_2.append(te_sub[KEYS].assign(pred_2=te_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e48bbd6f-c7ec-4e18-b3b4-395942db111e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['호텔', '상용', '병원', '학교', '건물기타', '아파트', '연구소', '백화점', 'IDC(전화국)',\n",
       "       '공공'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"건물유형\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cab9243-31cf-4e18-bedd-9dcef010c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[호텔] Validation SMAPE: 5.1850\n",
      "[상용] Validation SMAPE: 2.2325\n",
      "[병원] Validation SMAPE: 2.6472\n",
      "[학교] Validation SMAPE: 2.9510\n",
      "[건물기타] Validation SMAPE: 4.7772\n",
      "[아파트] Validation SMAPE: 4.6186\n",
      "[연구소] Validation SMAPE: 3.7907\n",
      "[백화점] Validation SMAPE: 4.9227\n",
      "[IDC(전화국)] Validation SMAPE: 0.8186\n",
      "[공공] Validation SMAPE: 4.4485\n"
     ]
    }
   ],
   "source": [
    "# ======================\n",
    "# 모델 3 — XGBoost (건물유형별)\n",
    "# ======================\n",
    "#피쳐임포턴스 반영\n",
    "drop_cols = ['T2', 'T_x_rain', 'RH2', '습도(%)', '풍속(m/s)', 'is_rain', 'wind_level', '강수량(mm)', '기온(°C)', 'ess_hours']\n",
    "building_types = list(train[\"건물유형\"].unique())\n",
    "for btype in building_types:\n",
    "    tr_sub = train[train[\"건물유형\"] == btype].copy()\n",
    "    te_sub = test [test [\"건물유형\"] == btype].copy()\n",
    "    if len(tr_sub) == 0:\n",
    "        continue\n",
    "\n",
    "    drop_cols_tr = BASE_DROP_COMMON + BASE_DROP_TRAIN + drop_cols\n",
    "    X_all = tr_sub.drop(columns=[c for c in drop_cols_tr if c in tr_sub.columns], errors=\"ignore\")\n",
    "    y_all = tr_sub[TARGET].astype(float)\n",
    "\n",
    "    # XGBoost는 float만 → object 수치 변환\n",
    "    X_all = coerce_numeric_objects(X_all).fillna(0.0)\n",
    "\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "    m3 = xgb.XGBRegressor(**params_xgb)\n",
    "    m3.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
    "\n",
    "    val_pred = m3.predict(X_val)\n",
    "\n",
    "    score = smape(y_val.values, val_pred)\n",
    "    val_scores_3[btype] = score   # 건물유형 단위 성능 기록\n",
    "    print(f\"[{btype}] Validation SMAPE: {score:.4f}\")\n",
    "\n",
    "    val_keys = tr_sub.loc[X_val.index, KEYS].reset_index(drop=True)\n",
    "    oof_df = val_keys.copy()\n",
    "    oof_df[\"y_true\"] = y_val.reset_index(drop=True).values\n",
    "    oof_df[\"pred_3\"] = val_pred\n",
    "    val_oofs_3.append(oof_df)\n",
    "\n",
    "    # TEST\n",
    "    X_test = te_sub.drop(columns=[c for c in BASE_DROP_COMMON + BASE_DROP_TEST if c in te_sub.columns], errors=\"ignore\")\n",
    "    X_test = coerce_numeric_objects(X_test).fillna(0.0)\n",
    "    X_test = reorder_like(X_test, X_all).fillna(0.0)\n",
    "    te_pred = m3.predict(X_test)\n",
    "    test_preds_3.append(te_sub[KEYS].assign(pred_3=te_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0b704ac-ccf3-49f1-a531-2aebd573fa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model 1 (CatBoost-건물유형) ===\n",
      "Validation SMAPE by type: {'호텔': np.float64(5.8292), '상용': np.float64(1.6775), '병원': np.float64(2.623), 'CLUSTER10': np.float64(7.6803), '학교': np.float64(2.5522), 'CLUSTER11': np.float64(5.5123), '건물기타': np.float64(4.1811), '아파트': np.float64(2.8773), '연구소': np.float64(2.8718), '백화점': np.float64(5.2003), 'IDC(전화국)': np.float64(1.1797), '공공': np.float64(5.1687)}\n",
      "평균 SMAPE: 3.9461\n",
      "\n",
      "=== Model 2 (CatBoost-cluster_id) ===\n",
      "Validation SMAPE by cluster: {'호텔': np.float64(5.5505), '상용': np.float64(1.7613), '병원': np.float64(2.5763), 'CLUSTER10': np.float64(5.1974), '학교': np.float64(2.8666), 'CLUSTER11': np.float64(5.5608), '건물기타': np.float64(4.1807), '아파트': np.float64(3.6995), '연구소': np.float64(3.1672), '백화점': np.float64(4.2196), 'IDC(전화국)': np.float64(0.7764), '공공': np.float64(5.0731)}\n",
      "평균 SMAPE: 3.7191\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m평균 SMAPE:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(np.mean(\u001b[38;5;28mlist\u001b[39m(val_scores_2.values())), \u001b[32m4\u001b[39m))\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 모델 3 결과 확인\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m final_preds_3 = pd.concat(test_preds_3, axis=\u001b[32m0\u001b[39m).sort_values([\u001b[33m'\u001b[39m\u001b[33mnum_date_time\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m건물번호\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Model 3 (XGBoost-건물유형) ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mValidation SMAPE by type:\u001b[39m\u001b[33m\"\u001b[39m, {k: \u001b[38;5;28mround\u001b[39m(v, \u001b[32m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m val_scores_3.items()})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/core/reshape/concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m    385\u001b[39m     ignore_index=ignore_index,\n\u001b[32m    386\u001b[39m     join=join,\n\u001b[32m    387\u001b[39m     keys=keys,\n\u001b[32m    388\u001b[39m     levels=levels,\n\u001b[32m    389\u001b[39m     names=names,\n\u001b[32m    390\u001b[39m     verify_integrity=verify_integrity,\n\u001b[32m    391\u001b[39m     copy=copy,\n\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/core/reshape/concat.py:445\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m objs, keys = \u001b[38;5;28mself\u001b[39m._clean_keys_and_objs(objs, keys)\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.13/site-packages/pandas/core/reshape/concat.py:507\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    504\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    510\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs_list))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# 모델 1 결과 확인\n",
    "final_preds_1 = pd.concat(test_preds_1, axis=0).sort_values(['num_date_time', '건물번호'])\n",
    "print(\"=== Model 1 (CatBoost-건물유형) ===\")\n",
    "print(\"Validation SMAPE by type:\", {k: round(v, 4) for k, v in val_scores_1.items()})\n",
    "print(\"평균 SMAPE:\", round(np.mean(list(val_scores_1.values())), 4))\n",
    "\n",
    "# 모델 2 결과 확인\n",
    "final_preds_2 = pd.concat(test_preds_2, axis=0).sort_values(['num_date_time', '건물번호'])\n",
    "print(\"\\n=== Model 2 (CatBoost-cluster_id) ===\")\n",
    "print(\"Validation SMAPE by cluster:\", {k: round(v, 4) for k, v in val_scores_2.items()})\n",
    "print(\"평균 SMAPE:\", round(np.mean(list(val_scores_2.values())), 4))\n",
    "\n",
    "# 모델 3 결과 확인\n",
    "final_preds_3 = pd.concat(test_preds_3, axis=0).sort_values(['num_date_time', '건물번호'])\n",
    "print(\"\\n=== Model 3 (XGBoost-건물유형) ===\")\n",
    "print(\"Validation SMAPE by type:\", {k: round(v, 4) for k, v in val_scores_3.items()})\n",
    "print(\"평균 SMAPE:\", round(np.mean(list(val_scores_3.values())), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed6144-7bc2-4cfc-b10a-2b53104f7e36",
   "metadata": {},
   "source": [
    "#### 기존  \n",
    "=== Model 1 (CatBoost-건물유형) ===  \n",
    "평균 SMAPE: 4.4898\n",
    "\n",
    "=== Model 2 (CatBoost-cluster_id) ===  \n",
    "평균 SMAPE: 2.6234\n",
    "\n",
    "=== Model 3 (XGBoost-건물유형) ===  \n",
    "V평균 SMAPE: 3.4165\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e14efded-55ca-4596-97e7-3b64f64c3a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OOF 기반 최적 가중치] w=(0.24, 0.76) | OOF SMAPE=4.0342\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 설정 ---\n",
    "# KEYS는 이미 전역에 있다고 하셨으니 그대로 사용합니다.\n",
    "# 예: KEYS = [\"num_date_time\"] 또는 [\"num_date_time\",\"건물번호\"] 등\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + 1e-8))\n",
    "\n",
    "def concat_if_any(lst):\n",
    "    return pd.concat(lst, ignore_index=True) if len(lst) else pd.DataFrame(columns=KEYS+[\"y_true\", \"pred\"])\n",
    "\n",
    "# 1) OOF 결합 (리스트 → 단일 DF) & 컬럼명 표준화\n",
    "oof1 = concat_if_any(val_oofs_1).rename(columns={\"pred\": \"pred_1\"})\n",
    "oof2 = concat_if_any(val_oofs_2).rename(columns={\"pred\": \"pred_2\"})\n",
    "\n",
    "# 2) 동일 샘플만 비교하도록 inner merge\n",
    "need_cols = KEYS + [\"y_true\"]\n",
    "assert all(c in oof1.columns for c in need_cols+[\"pred_1\"]), \"oof1 컬럼 확인 필요\"\n",
    "assert all(c in oof2.columns for c in need_cols+[\"pred_2\"]), \"oof2 컬럼 확인 필요\"\n",
    "\n",
    "val_join = oof1.merge(oof2, on=KEYS+[\"y_true\"], how=\"inner\").copy()\n",
    "assert len(val_join) > 0, \"OOF 조인 결과가 비었습니다. KEYS/y_true 일치 여부를 확인하세요.\"\n",
    "\n",
    "# 3) 가중치 최적화 (Σw=1, w>=0) — 모델 2개\n",
    "y_true = val_join[\"y_true\"].values\n",
    "p1 = val_join[\"pred_1\"].values\n",
    "p2 = val_join[\"pred_2\"].values\n",
    "\n",
    "grid = np.linspace(0, 1, 51)  # 0.02 step\n",
    "best = {\"w\": (0.5, 0.5), \"smape\": 1e9}\n",
    "\n",
    "for w1 in grid:\n",
    "    w2 = 1.0 - w1\n",
    "    y_hat = w1 * p1 + w2 * p2\n",
    "    s = smape(y_true, y_hat)\n",
    "    if s < best[\"smape\"]:\n",
    "        best = {\"w\": (float(w1), float(w2)), \"smape\": float(s)}\n",
    "\n",
    "print(f\"[OOF 기반 최적 가중치] w={best['w']} | OOF SMAPE={best['smape']:.4f}\")\n",
    "\n",
    "# 4) test 예측 결합\n",
    "# 각 리스트를 하나로 합치고, KEYS + answer만 남긴 뒤 inner merge로 정렬/정합성 보장\n",
    "def concat_test_preds(lst, answer_col=\"answer\", alias=None):\n",
    "    \"\"\"\n",
    "    - lst: 여러 조각의 test 예측 DataFrame 리스트\n",
    "    - answer_col: 기본적으로 기대하는 예측 컬럼명 (ex. 'answer')\n",
    "    - alias: 반환 시 예측 컬럼을 이 이름으로 맞춰줌 (ex. 'ans_1', 'ans_2')\n",
    "    \"\"\"\n",
    "    if len(lst) == 0:\n",
    "        # 빈 리스트면 빈 DF 반환\n",
    "        col = alias if alias else answer_col\n",
    "        return pd.DataFrame(columns=KEYS + [col])\n",
    "\n",
    "    df = pd.concat(lst, ignore_index=True)\n",
    "\n",
    "    # 1) 예측 컬럼 자동 탐지\n",
    "    candidates = [answer_col, \"pred\", \"prediction\", \"y_pred\", \"ans\", \"value\"]\n",
    "    pred_col = next((c for c in candidates if c in df.columns), None)\n",
    "\n",
    "    # 2) 그래도 못 찾으면, KEYS를 제외한 단 하나의 컬럼이 예측이라고 간주\n",
    "    if pred_col is None:\n",
    "        non_key_cols = [c for c in df.columns if c not in KEYS]\n",
    "        if len(non_key_cols) == 1:\n",
    "            pred_col = non_key_cols[0]\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                f\"예측 컬럼을 찾지 못했습니다. 후보 {candidates} 중 하나가 있어야 하거나, \"\n",
    "                f\"KEYS({KEYS}) 제외 단일 컬럼만 남아야 합니다. 현재 컬럼: {list(df.columns)}\"\n",
    "            )\n",
    "\n",
    "    # 3) alias로 이름 통일\n",
    "    out_col = alias if alias else answer_col\n",
    "    if pred_col != out_col:\n",
    "        df = df.rename(columns={pred_col: out_col})\n",
    "\n",
    "    # 4) 필요한 컬럼만 반환 (정합성 보장)\n",
    "    need_cols = KEYS + [out_col]\n",
    "    missing = [c for c in need_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"필요 컬럼 누락: {missing}. 현재 컬럼: {list(df.columns)}\")\n",
    "\n",
    "    return df[need_cols]\n",
    "\n",
    "test1 = concat_test_preds(test_preds_1, answer_col=\"answer\", alias=\"ans_1\")\n",
    "test2 = concat_test_preds(test_preds_2, answer_col=\"answer\", alias=\"ans_2\")\n",
    "\n",
    "# 정합성 체크 후 조인\n",
    "test_join = test1.merge(test2, on=KEYS, how=\"inner\").copy()\n",
    "\n",
    "w1, w2 = best[\"w\"]\n",
    "test_join[\"answer\"] = w1 * test_join[\"ans_1\"].values + w2 * test_join[\"ans_2\"].values\n",
    "\n",
    "submission = test_join[KEYS + [\"answer\"]].copy()\n",
    "# submission[\"answer\"] = submission[\"answer\"].clip(lower=0)  # 필요 시\n",
    "# submission.to_csv(\"submit_blend_1_2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21df2051-406b-4b7b-b0eb-9544ce39ffa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_date_time</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_20240825 00</td>\n",
       "      <td>4611.274684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_20240825 01</td>\n",
       "      <td>4209.557334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_20240825 02</td>\n",
       "      <td>3988.090061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_20240825 03</td>\n",
       "      <td>3627.912793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_20240825 04</td>\n",
       "      <td>3364.295444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16795</th>\n",
       "      <td>100_20240831 19</td>\n",
       "      <td>2240.111938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16796</th>\n",
       "      <td>100_20240831 20</td>\n",
       "      <td>2352.154591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16797</th>\n",
       "      <td>100_20240831 21</td>\n",
       "      <td>2214.481248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16798</th>\n",
       "      <td>100_20240831 22</td>\n",
       "      <td>2441.752701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16799</th>\n",
       "      <td>100_20240831 23</td>\n",
       "      <td>2781.147893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         num_date_time       answer\n",
       "0        1_20240825 00  4611.274684\n",
       "1        1_20240825 01  4209.557334\n",
       "2        1_20240825 02  3988.090061\n",
       "3        1_20240825 03  3627.912793\n",
       "4        1_20240825 04  3364.295444\n",
       "...                ...          ...\n",
       "16795  100_20240831 19  2240.111938\n",
       "16796  100_20240831 20  2352.154591\n",
       "16797  100_20240831 21  2214.481248\n",
       "16798  100_20240831 22  2441.752701\n",
       "16799  100_20240831 23  2781.147893\n",
       "\n",
       "[16800 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) 제출 형식으로 정리 (예: num_date_time, answer)\n",
    "submission = test_join[KEYS + [\"answer\"]].copy()\n",
    "\n",
    "submit = pd.read_csv('Data/sample_submission.csv')\n",
    "\n",
    "sub_out = submit.merge(\n",
    "    submission[['num_date_time', 'answer']],\n",
    "    on='num_date_time',\n",
    "    how=\"left\"\n",
    ").copy()\n",
    "\n",
    "# 컬럼 이름 변경\n",
    "sub_out = sub_out[['num_date_time', 'answer_y']].rename(columns={'answer_y': 'answer'})\n",
    "\n",
    "# 최종 제출 파일 저장\n",
    "sub_out.to_csv(\"Submit/submit0822_newcluster.csv\", index=False)\n",
    "sub_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c314fd7-0e28-427b-834c-b0e81fc9f7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8f3c484-a721-4762-a856-8f7915730e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_min_clip_by_weekday_hour(train: pd.DataFrame,\n",
    "                                   test: pd.DataFrame,\n",
    "                                   preds_df: pd.DataFrame,\n",
    "                                   pred_col: str = \"pred_ens\",\n",
    "                                   target_col: str = \"전력소비량(kWh)\") -> pd.DataFrame:\n",
    "    \"\"\"6-8월 train에서 (건물번호, 요일, 시)별 타깃의 '최저값'을 구해\n",
    "       예측이 그 값보다 작으면 교체(상향)한다.\n",
    "    \"\"\"\n",
    "    # 1) train에서 요일/시/월 파생\n",
    "    tr = train.copy()\n",
    "    tr[\"dt\"] = pd.to_datetime(tr[\"일시\"])\n",
    "    tr[\"month\"] = tr[\"dt\"].dt.month\n",
    "    tr[\"weekday\"] = tr[\"dt\"].dt.dayofweek\n",
    "    tr[\"hour\"] = tr[\"dt\"].dt.hour\n",
    "\n",
    "    # 2) 6~8월만 필터\n",
    "    tr_summer = tr[(tr[\"month\"].between(6, 8))].copy()\n",
    "\n",
    "    # 3) (건물번호, weekday, hour)별 최저 타깃\n",
    "    grp_min = (tr_summer\n",
    "               .groupby([\"건물번호\", \"weekday\", \"hour\"], as_index=False)[target_col]\n",
    "               .min()\n",
    "               .rename(columns={target_col: \"min_hist\"}))\n",
    "\n",
    "    # 4) test에도 요일/시 파생\n",
    "    te = test[[\"num_date_time\", \"건물번호\", \"일시\"]].copy()\n",
    "    te[\"dt\"] = pd.to_datetime(te[\"일시\"])\n",
    "    te[\"weekday\"] = te[\"dt\"].dt.dayofweek\n",
    "    te[\"hour\"] = te[\"dt\"].dt.hour\n",
    "    te = te[[\"num_date_time\", \"건물번호\", \"weekday\", \"hour\"]]\n",
    "\n",
    "    # 5) preds_df와 key merge\n",
    "    base = preds_df.merge(te, on=[\"num_date_time\", \"건물번호\"], how=\"left\")\n",
    "\n",
    "    # 6) (건물번호, weekday, hour)로 min_hist merge\n",
    "    base = base.merge(grp_min, on=[\"건물번호\", \"weekday\", \"hour\"], how=\"left\")\n",
    "\n",
    "    # 7) 하한 클리핑\n",
    "    before = base[pred_col].copy()\n",
    "    base[pred_col] = np.where(\n",
    "        (~base[\"min_hist\"].isna()) & (base[pred_col] < base[\"min_hist\"]),\n",
    "        base[\"min_hist\"],\n",
    "        base[pred_col]\n",
    "    )\n",
    "\n",
    "    # 차이 계산\n",
    "    diff = (base[pred_col] - before).clip(lower=0)  # 교체된 만큼의 증가량\n",
    "    changed = int((diff > 0).sum())\n",
    "    total_increase = diff.sum()\n",
    "\n",
    "    print(f\"[Clip] {changed} rows were raised. 누적 증가량={total_increase:.2f}\")\n",
    "\n",
    "    return base[preds_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "267d571a-8092-46e9-9195-c2bd83662f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clip] 125 rows were raised. 누적 증가량=16409.42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_date_time</th>\n",
       "      <th>건물번호</th>\n",
       "      <th>pred_ens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100_20240825 00</td>\n",
       "      <td>100</td>\n",
       "      <td>2861.548567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100_20240825 01</td>\n",
       "      <td>100</td>\n",
       "      <td>2735.605156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100_20240825 02</td>\n",
       "      <td>100</td>\n",
       "      <td>2526.012037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100_20240825 03</td>\n",
       "      <td>100</td>\n",
       "      <td>2097.608341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100_20240825 04</td>\n",
       "      <td>100</td>\n",
       "      <td>2390.432590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16795</th>\n",
       "      <td>9_20240831 19</td>\n",
       "      <td>9</td>\n",
       "      <td>2960.160710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16796</th>\n",
       "      <td>9_20240831 20</td>\n",
       "      <td>9</td>\n",
       "      <td>2783.665425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16797</th>\n",
       "      <td>9_20240831 21</td>\n",
       "      <td>9</td>\n",
       "      <td>2656.394903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16798</th>\n",
       "      <td>9_20240831 22</td>\n",
       "      <td>9</td>\n",
       "      <td>2333.739335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16799</th>\n",
       "      <td>9_20240831 23</td>\n",
       "      <td>9</td>\n",
       "      <td>2159.093354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         num_date_time  건물번호     pred_ens\n",
       "0      100_20240825 00   100  2861.548567\n",
       "1      100_20240825 01   100  2735.605156\n",
       "2      100_20240825 02   100  2526.012037\n",
       "3      100_20240825 03   100  2097.608341\n",
       "4      100_20240825 04   100  2390.432590\n",
       "...                ...   ...          ...\n",
       "16795    9_20240831 19     9  2960.160710\n",
       "16796    9_20240831 20     9  2783.665425\n",
       "16797    9_20240831 21     9  2656.394903\n",
       "16798    9_20240831 22     9  2333.739335\n",
       "16799    9_20240831 23     9  2159.093354\n",
       "\n",
       "[16800 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clipped_preds = apply_min_clip_by_weekday_hour(train, test, test_join[[\"num_date_time\",'건물번호',\"pred_ens\"]], pred_col=\"pred_ens\")\n",
    "clipped_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "767d197a-a64b-456c-96ef-c46055960930",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clipped_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m submit = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mData/sample_submission.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m sub_out = submit.merge(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     clipped_preds[[\u001b[33m'\u001b[39m\u001b[33mnum_date_time\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpred_ens\u001b[39m\u001b[33m'\u001b[39m]],\n\u001b[32m      5\u001b[39m     on=\u001b[33m'\u001b[39m\u001b[33mnum_date_time\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m     how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m ).copy()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 컬럼 이름 변경\u001b[39;00m\n\u001b[32m     10\u001b[39m sub_out = sub_out[[\u001b[33m'\u001b[39m\u001b[33mnum_date_time\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpred_ens\u001b[39m\u001b[33m'\u001b[39m]].rename(columns={\u001b[33m'\u001b[39m\u001b[33mpred_ens\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m})\n",
      "\u001b[31mNameError\u001b[39m: name 'clipped_preds' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22fc0c-edeb-4fbc-86f4-edd1ae082c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
